2017/01/15
I found the "Relation Extraction - Perspective from Convolutional Neural Networks" paper and decided that this would be
the classifier I would use. This python project is for using the implementations of that paper I found online and
modify that to serve the purpose of extracting relations from neural science and computer science papers.

The pipeline would be as follows:

1. Preprocess unlabeled data and obtain training data:
    The first step is to clean up the raw text.
        I already did this part in the 2016 fall quarter.

    Use the list of key phrases I got to obtain the unlabeled data.
        If two phrases appears within 15 tokens apart from each other, I take the sentence that they're in and record
        the sentence as well as the position of the two phrases.

        This part is also largely done already

    Obtain training data
        Done last quarter, but need some more work because now the requirement is different.

    Padding and turning into embedding
        There are 3 relationships, isa(x1,x2), isa(x2,x1), and Other.
        We can specify a fixed sentence length and pad the sentence with <pad>
        Embedding composes of: 1. word embedding, either from pretrained or random vectors. m_e = 300 2. position
        embedding vectors with dimensionality of m_d = 50. Each word would have two of those indicating their relative
        position to the two target words.
        The embeddings are resized when its l2 exceed 3.

    Dataset:
        The dataset now has shape: X = num_instances x sentence_length x (m_e + 2 * m_d), Y = num_instances x 3

2. Active learning


2017/01/16
    I finished implementing the preprocessing pipeline. The problem is that the key phrases are a little bit too
    noisy. I think I got less than 10 positives in 500 sentences that I labeled.

2017/01/18
    Key phrases are too noisy. I need a named entity recognition thing... or anything that label key words/phrases for
    me.

2017/01/23
    I came back to the "such as" phrase again just because the key phrases are way too noisy. But now I'm stuck with
    having a good definition of an "is-a" relation. Should I mark it as true only if it make sense in the context of
    its sentence, or if it's true in general?
    ... Again, I need a large dataset. I can get one by manually marking key phrases around "such as", but that would
    only bias the model towards marking anything around "such as" as positive. Making dataset is such a pain. I've
    marked around 70 instances with "such as" and I can do faster without "such as". But what I need was something on
    the order of thousands. I can mix the "such as" and without "such as" together to create a larger set of sentences,
    then I mark all of them... Sounds like a plan. It will take a while, but I have no choice.

    Plan: 1. Mark the unlabeled dataset
    Plan: 2. Rerun the preprocessing util on both "such as" and without and combine them to get a large enough dataset.
    (Done)

2017/01/24
    I wrote the program to combine datasets. Now I have a dataset with 900 labeled sentences. It's working to some
    extent now after enough epochs of training. I'll keep labeling more data.
    Plan: 1. Mark the unlabeled dataset
    Plan: 2. Find pretrained embedding and use that to generate embedding for the key phrases. (Done)
    Plan: 3. Active learning.
    Plan: 4 Find a easy way to modify the labeled dataset since it may contain mislabeled items.

2017/01/26
    I added functions to easily draw precision recall curves.

2017/01/28
    Started to add the active learning module.
    Active learning module done. Now it's just a matter of the size of the dataset and how to accuractely label data
    and/or find out what are the ones likely to be marked wrong.
    That can be achieved by cross validation I think. If I always split the training set into half and do that randomly
    100 times. Each time I predict the label of one instance in the test set and it is always wrong, then it is likely
    that the instance is really different from anything else.
    Of course really doing that might be too costly, so maybe I can think of something else if I have time.

2017/01/29
    Working on writing a function that takes the two key words, find all their occurrence in labeled dataset, and
    modify one or all of them. Done.

2017/02/01
    Half of the website is done. The only part left is converting user labels to actual labels that can be used by the
    cnn, while keeping the record for who labeled what. I also need to support training a new cnn from the newly
    labeled while the user is labeling new instances at the same time. I need to switch from one model to the other
    seamlessly, or take as little time as possible.
    Lastly I need more unlabeled dataset.
    That should keep me busy for tomorrow.

2017/02/01
    The website is up and running. Things left are:
        When the user enter their user name, do not show the options.
        Force user to choose one option before sending a request to the server.
        Record the user's reason.
        Fix the submit button ui.

    All the tasks above are done. I tested it and it worked fine.
    Here's the response last time I left off. It should be different in an hour.

    solvers grasp

    several sat solvers* such as grasp* [ marques-silva and sakallah 1999 ] and chaff [ moskewicz et al .

2017/02/03
    TODO: I should probably delete the emb file after I'm done using them.

2017/03/11
wrong labels:
2.000	A is-a B	<bos> observed symptoms* and ( ) of each probe are initialized at the beginning of the active probing* . <eos>		[ 1.  0.  0.]	[  5.84049303e-07   1.64866105e-05   9.99982953e-01]
2.000	A is-a B	<bos> for the external i/o we multiplexed the i/o buses* to reduce the number of pins* . <eos>		[ 1.  0.  0.]	[  6.50761285e-05   1.21757612e-05   9.99922812e-01]
1.997	A is-a B	<bos> figure 2 shows the prior and the posterior* distributions of the coefficients* , using mcmc with 10000 iterations . <eos>		[ 1.  0.  0.]	[ 0.00169531  0.00902555  0.98927915]

1.998	A is-a B	<bos> the network_database actually holds a representation of the social network ; it may be in-memory or on persistent* storage* . <eos>		[ 1.  0.  0.]	[  7.67719117e-04   9.74751770e-01   2.44805720e-02]
I'm pretty sure there's something wrong in the database. I think...

2017/03/12
The reason why people labeled things as it is varies a lot. Some think that an "is-a" relation can be used on
adjectives as well. Some think if the relation does not make sense in the sentence then it should not count.
Some think the other way around. It will require lots of cleaning up but it is definitely useful to see how others view
this very subjective problem.

2017/03/27
I want to summarize what I have done in the past quarters.

2017/03/30
I will separate the problem into two tasks. One is replace keywords with dollar signs and let the classifier just
classify based on the sentence, not the key phrases. This will force the classifier to learn from sentences, not just
use the word embeddings. This will avoid having the problem of labeling every instance with “Intel” in it as True.

2017/04/02
TODO: Pick some phrase in the extracted context ratio data, and examine all instances containing that phrase to see
if the context ratio is actually helpful. (Because the ratio is pretty low, lower than I thought.)
TODO: Clean up the database. Double check each one labeled.
TODO: For each label, go through the dataset and label all instances with the same key phrase pair as positive/negative.
If there is a conflict, debug it.

2017/04/03
After the meeting:
So the pipeline would be like:
Start with what I labeled, get all sentences with the labeled key phrases, and train the sentence level classifier.
Pick the most confident ones labeled by the sentence classifier and feed the corresponding key phrases to the key
phrase classifier.
Train the key phrase classifier and label more key phrase pairs. Pick the most confident one in those and get the
sentences they occurred in.
Feed the additional sentences back into the sentence level classifier and get some more key phrases. And the cycle
completes.

I was not sure whether I should go through the key phrase classifier. Because Doug seems to think the key phrase
classifier will just learn to memorize but not label anything new. So he suggested directly taking the most confident
ones and treat them as extra training data, without going through the key phrase classifier.

I can read the paper again and try both.

Updating the to-do list after the meeting:
TODO: get all sentences with the labeled key phrases
TODO: Pick the most confident ones labeled by the sentence classifier and feed the corresponding key phrases to the key
phrase classifier. (Can do so manually for now)
    I have to modify the current code so that it outputs the labeled ones with the highest confidence not in txt format
    but in the format of dataset.

TODO: Build a key phrase classifier.

2017/04/04
    I finished the pipeline to alternate between training and labeling unlabeled data. The problem is there is way too
    many instances of "neither" relation than "a-isa-b" or "b-isa-a". I can of course use two different thresholds for
    them. But even if I set the threshold to 0.9999 there is still 1/2 of unlabeled data being labeled as "neither",
    and some of them are wrongly labeled.
    Yeah the pipeline works but the quality is low. It is labeling too many instances because the model is over
    confident (couldn't blame it really because it is trained to output either 0 or 1.
    I should implement the key phrase classifier. I should also re-read the paper.
